{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to calculate eye aspect ratio (EAR)\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Indices of facial landmarks for left and right eyes\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "# Function to calculate eye aspect ratio\n",
    "def eye_aspect_ratio(eye):\n",
    "    eye = np.array(eye)\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to process frames\n",
    "def process_frame(frame, frame_count, output_dir):\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Noise reduction using bilateral filter\n",
    "    blurred = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    \n",
    "    # Histogram equalization for enhancing contrast\n",
    "    equalized = cv2.equalizeHist(gray)\n",
    "    \n",
    "    # Edge detection using Canny\n",
    "    # edges = cv2.Canny(gray, 100, 20)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    edges = cv2.sqrt(cv2.addWeighted(cv2.pow(sobelx, 2), 1.0, cv2.pow(sobely, 2), 1.0, 0))\n",
    "    edges = cv2.normalize(edges, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    \n",
    "    # Morphology techniques (Closing operation to close gaps between edges)\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    closing = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # improve contrast using open cv scal abs\n",
    "    contrasted = cv2.convertScaleAbs(gray, alpha=1.5, beta=0)\n",
    "    \n",
    "    # Eye detection using dlib\n",
    "    subjects = detector(equalized, 0)\n",
    "    leftEye = []\n",
    "    rightEye = []\n",
    "    for subject in subjects:\n",
    "        shape = predictor(equalized, subject)\n",
    "        shape = [(shape.part(i).x, shape.part(i).y) for i in range(68)]\n",
    "        \n",
    "        # Extract left and right eye landmarks\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "\n",
    "        # Calculate eye aspect ratio if both left and right eyes are detected\n",
    "        if len(leftEye) == 6 and len(rightEye) == 6:\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            \n",
    "            # Average EAR of both eyes\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "        else:\n",
    "            ear = 0.0\n",
    "    \n",
    "    # Save processed frames into respective directories\n",
    "    cv2.imwrite(os.path.join(output_dir, 'original', f'frame-{frame_count}.jpg'), frame)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'sobel', f'frame-{frame_count}.jpg'), edges)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'grey', f'frame-{frame_count}.jpg'), gray)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'morphology', f'frame-{frame_count}.jpg'), closing)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'noise_reduction', f'frame-{frame_count}.jpg'), blurred)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'hist_equilization', f'frame-{frame_count}.jpg'), equalized)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'contrast_improv', f'frame-{frame_count}.jpg'), contrasted)\n",
    "    \n",
    "    return equalized\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "output_dir = 'output'\n",
    "os.makedirs(os.path.join(output_dir, 'original'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'sobel'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'grey'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'morphology'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'noise_reduction'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'hist_equilization'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'contrast_improv'), exist_ok=True)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('videos/video3.MOV')\n",
    "\n",
    "# Initialize variables\n",
    "frame_count = 0\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "interval = int(fps) * 1  # Extract frame every 1 second\n",
    "\n",
    "# Process frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    if frame_count % interval == 0:\n",
    "        processed_frame = process_frame(frame, frame_count, output_dir)\n",
    "        cv2.imshow('Processed Frame', processed_frame)\n",
    "        \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(r'output\\original\\frame-87.jpg')\n",
    "\n",
    "# Convert the image to HSV color space\n",
    "hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define range of eye color in HSV\n",
    "lower_eye_color = np.array([0, 50, 50])\n",
    "upper_eye_color = np.array([30, 255, 255])\n",
    "\n",
    "# Threshold the HSV image to get only eye color\n",
    "mask = cv2.inRange(hsv_image, lower_eye_color, upper_eye_color)\n",
    "\n",
    "# Bitwise-AND mask and original image\n",
    "segmented_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "# Display the segmented image\n",
    "\n",
    "# change the size of the window output\n",
    "cv2.namedWindow('Segmented Eyes', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Segmented Eyes', 1080, 720)\n",
    "cv2.imshow('Segmented Eyes', segmented_image)\n",
    "# cv2.imshow('Segmented Eyes', segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils\n",
    "import pygame\n",
    "from threading import Thread\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Initialize pygame mixer for playing audio and Load the alert sound\n",
    "pygame.mixer.init()\n",
    "alert_sound = pygame.mixer.Sound(\"audio/wake_up.wav\")\n",
    "danger_image = cv2.imread(\"assets/danger.png\")\n",
    "\n",
    "# Function to calculate eye aspect ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "                \n",
    "def save_ear_values(ear_values, frame_count, threshold=0.25):\n",
    "    if ear_values:\n",
    "        if frame_count % 10 == 0:\n",
    "            timestamp = datetime.now().strftime(\"%M:%S:%f\")\n",
    "            with open('output/ear_values.csv', mode='a', newline='\\n') as file:\n",
    "                writer = csv.writer(file)\n",
    "                if file.tell() == 0:\n",
    "                    writer.writerow(['time', 'EAR'])\n",
    "                for ear in ear_values:  # Iterate over all EAR values\n",
    "                    writer.writerow([timestamp, ear])  # Save each EAR value with the timestamp\n",
    "            \n",
    "            with open('output/tiredness.csv', mode='a', newline='\\n') as tiredness_file:\n",
    "                tiredness_writer = csv.writer(tiredness_file)\n",
    "                if tiredness_file.tell() == 0:\n",
    "                    tiredness_writer.writerow(['time', 'EAR'])\n",
    "                \n",
    "                for ear in ear_values:  # Iterate over all EAR values\n",
    "                    if ear < threshold:  # Check if the EAR value is below the threshold\n",
    "                        tiredness_writer.writerow([timestamp, 1])  # Save the EAR value if below the threshold\n",
    "                    else:\n",
    "                        tiredness_writer.writerow([timestamp, 0])\n",
    "\n",
    "# Initialize variables\n",
    "ear_values = []\n",
    "frame_count = 0\n",
    "frame_count_2 = 0\n",
    "alert_count = 0\n",
    "exit_flag = False\n",
    "\n",
    "# Load face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Indices of facial landmarks for left and right eyes\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "\n",
    "# Function to process frames\n",
    "def process_frame(frame, frame_count, output_dir):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Noise reduction using bilateral filter\n",
    "    blurred = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    \n",
    "    # Histogram equalization for enhancing contrast\n",
    "    equalized = cv2.equalizeHist(gray)\n",
    "    \n",
    "    # Edge detection using Sobel\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    edges = cv2.sqrt(cv2.addWeighted(cv2.pow(sobelx, 2), 1.0, cv2.pow(sobely, 2), 1.0, 0))\n",
    "    edges = cv2.normalize(edges, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    \n",
    "    # Morphology techniques (Closing operation to close gaps between edges)\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    closing = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Improve contrast using OpenCV convertScaleAbs\n",
    "    contrasted = cv2.convertScaleAbs(gray, alpha=1.5, beta=0)\n",
    "    \n",
    "    # Save processed frames into respective directories\n",
    "    cv2.imwrite(os.path.join(output_dir, 'original', f'frame-{frame_count}.jpg'), frame)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'sobel', f'frame-{frame_count}.jpg'), edges)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'grey', f'frame-{frame_count}.jpg'), gray)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'morphology', f'frame-{frame_count}.jpg'), closing)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'noise_reduction', f'frame-{frame_count}.jpg'), blurred)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'hist_equilization', f'frame-{frame_count}.jpg'), equalized)\n",
    "    cv2.imwrite(os.path.join(output_dir, 'contrast_improv', f'frame-{frame_count}.jpg'), contrasted)\n",
    "    \n",
    "    return gray\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "output_dir = 'output'\n",
    "os.makedirs(os.path.join(output_dir, 'original'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'sobel'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'grey'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'morphology'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'noise_reduction'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'hist_equilization'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'contrast_improv'), exist_ok=True)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "width = 1080\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Initialize variables\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "interval = int(fps) * 1  # Extract frame every 1 second\n",
    "\n",
    "# Function to process frames and save processed frames every 1 second\n",
    "def process_frames_and_save():\n",
    "    global frame_count, exit_flag  # Declare frame_count and exit_flag as global\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or exit_flag:  # Exit loop if not ret or exit_flag is True\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        if frame_count % interval == 0:\n",
    "            processed_frame = process_frame(frame, frame_count, output_dir)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            exit_flag = True  # Set exit_flag to True if 'q' is pressed\n",
    "            break\n",
    "\n",
    "# Start a thread for processing frames and saving processed frames\n",
    "processing_thread = Thread(target=process_frames_and_save)\n",
    "processing_thread.start()\n",
    "\n",
    "# Start detecting tiredness\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocessing: Convert to grayscale, detect faces\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    subjects = detector(gray, 0)\n",
    "\n",
    "    # Loop over detected faces\n",
    "    for subject in subjects:\n",
    "        # Detect facial landmarks\n",
    "        shape = predictor(gray, subject)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # Extract left and right eye coordinates\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "\n",
    "        # Calculate EAR for each eye\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        # Average EAR of both eyes\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "        ear_values.append(ear)\n",
    "        \n",
    "        # Draw eyes contours\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 255), 1)  # Yellow color\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 255), 1)  # Yellow color\n",
    "\n",
    "        # Draw eyes on the frame\n",
    "        for (x, y) in leftEye:\n",
    "            cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n",
    "        for (x, y) in rightEye:\n",
    "            cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n",
    "\n",
    "        # Connect landmarks with lines\n",
    "        cv2.line(frame, tuple(leftEye[1]), tuple(leftEye[5]), (0, 255, 0), 1)\n",
    "        cv2.line(frame, tuple(leftEye[2]), tuple(leftEye[4]), (0, 255, 0), 1)\n",
    "        cv2.line(frame, tuple(leftEye[0]), tuple(leftEye[3]), (0, 255, 0), 1)\n",
    "        cv2.line(frame, tuple(rightEye[1]), tuple(rightEye[5]), (0, 255, 0), 1)\n",
    "        cv2.line(frame, tuple(rightEye[2]), tuple(rightEye[4]), (0, 255, 0), 1)\n",
    "        cv2.line(frame, tuple(rightEye[0]), tuple(rightEye[3]), (0, 255, 0), 1)\n",
    "\n",
    "        # Check if eyes are closed\n",
    "        if ear < 0.25:\n",
    "            alert_count += 1\n",
    "            if alert_count >= 10 and not pygame.mixer.get_busy():\n",
    "                # Visual alert (draw text)\n",
    "                cv2.putText(frame, \"ALERT! Fatigue Detected !!!\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 4)\n",
    "                cv2.putText(frame, \"ALERT! Fatigue Detected !!!\", (frame.shape[1] - 550, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 4)\n",
    "\n",
    "                # Display danger image on both sides\n",
    "                frame[50:50 + danger_image.shape[0], 50:50 + danger_image.shape[1]] = danger_image\n",
    "                frame[50:50 + danger_image.shape[0], frame.shape[1] - 50 - danger_image.shape[1]:frame.shape[1] - 50] = danger_image\n",
    "\n",
    "                # Play sound alert\n",
    "                alert_sound.play()\n",
    "\n",
    "        else:\n",
    "            alert_count = 0\n",
    "\n",
    "    # Display the frame (if needed)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        exit_flag = True\n",
    "        break\n",
    "\n",
    "    # Increment frame count\n",
    "    frame_count_2 += 1\n",
    "    \n",
    "    # Check if it's time to save EAR values to CSV\n",
    "    if frame_count_2 % 10 == 0:  # Save EAR values every 10 frames\n",
    "        save_ear_values(ear_values, frame_count_2)\n",
    "        ear_values = []  # Clear the ear_values list\n",
    "        \n",
    "# Wait for the processing thread to finish\n",
    "processing_thread.join()\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Load the pre-trained facial landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imread(\"pictures/face.png\")\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the grayscale image\n",
    "faces = detector(gray)\n",
    "\n",
    "# Iterate over each detected face\n",
    "for face in faces:\n",
    "    # Predict facial landmarks for the detected face\n",
    "    landmarks = predictor(gray, face)\n",
    "    landmarks = [(p.x, p.y) for p in landmarks.parts()]\n",
    "\n",
    "    # Draw circles at each landmark point\n",
    "    for i, landmark in enumerate(landmarks, start=1):\n",
    "        cv2.circle(image, landmark, 5, (0, 255, 0), -1)  # Red color\n",
    "        cv2.putText(image, str(i), landmark, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)  # Green color\n",
    "\n",
    "    # Connect landmarks with lines\n",
    "    # for i in range(1, len(landmarks)):\n",
    "    #     cv2.line(image, landmarks[i - 1], landmarks[i], (255, 0, 0), 1)  # Blue color\n",
    "\n",
    "# Save the image with landmarks\n",
    "cv2.imwrite(\"output_image.jpg\", image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fatigue-detection-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
